{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa87315",
   "metadata": {},
   "source": [
    "# TP03: Testing and Collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88abfca",
   "metadata": {},
   "source": [
    "## Exercise 1: Unit Test for Data Cleaning\n",
    "**Objective**: Practice writing unit tests with pytest.\n",
    "\n",
    "**Task**: Write tests for a `clean_data(df)` function that removes duplicates and nulls.\n",
    "\n",
    "- Duplicates are removed correctly.\n",
    "- All null values are dropped.\n",
    "- The number of rows decreases after cleaning when nulls or duplicates exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215e6c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cleaning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cleaning.py\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Removes duplicates and null values from the DataFrame.\n",
    "    \"\"\"\n",
    "    df_clean = df.drop_duplicates()\n",
    "    df_clean = df_clean.dropna()\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e49dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_cleaning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_cleaning.py\n",
    "import pandas as pd\n",
    "import pytest\n",
    "from cleaning import clean_data\n",
    "\n",
    "def test_clean_data():\n",
    "    # Create sample data with duplicates and nulls\n",
    "    data = {\n",
    "        \"id\": [1, 2, 2, 3, 4],\n",
    "        \"value\": [10, 20, 20, None, 40]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Apply cleaning\n",
    "    df_cleaned = clean_data(df)\n",
    "    \n",
    "    # Assertions\n",
    "    assert df_cleaned.shape[0] == 3, \"Should have 3 rows after cleaning (removed 1 duplicate and 1 null)\"\n",
    "    assert df_cleaned.isnull().sum().sum() == 0, \"Should have no null values\"\n",
    "    assert df_cleaned.duplicated().sum() == 0, \"Should have no duplicates\"\n",
    "    assert 2 in df_cleaned[\"id\"].values, \"ID 2 should still exist\"\n",
    "    assert 3 not in df_cleaned[\"id\"].values, \"ID 3 (with null value) should be removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f72961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /Users/macbookair/Documents/Data Science 5th Year/Advanced Programing for DS\n",
      "plugins: anyio-4.9.0, langsmith-0.4.8\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_cleaning.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.54s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_cleaning.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.54s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_cleaning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0395a",
   "metadata": {},
   "source": [
    "## Exercise 2: TDD - Normalization Function\n",
    "**Objective**: Apply Test-Driven Development (TDD).\n",
    "\n",
    "**Task**:\n",
    "1. Write tests first for a function `normalize_column(df, column)` that scales values between 0 and 1.\n",
    "2. Implement the function to make the tests pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba9b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing normalization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile normalization.py\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_column(df, column):\n",
    "    \"\"\"\n",
    "    Scales values in the specified column between 0 and 1.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column {column} not found in DataFrame\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    min_val = df_copy[column].min()\n",
    "    max_val = df_copy[column].max()\n",
    "    \n",
    "    if max_val - min_val == 0:\n",
    "        df_copy[column] = 0.0\n",
    "    else:\n",
    "        df_copy[column] = (df_copy[column] - min_val) / (max_val - min_val)\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258ba560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_normalization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_normalization.py\n",
    "import pandas as pd\n",
    "import pytest\n",
    "# We import the function even though it might not exist yet (TDD process)\n",
    "# In a real TDD cycle, this import would fail first.\n",
    "try:\n",
    "    from normalization import normalize_column\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def test_normalize_column():\n",
    "    df = pd.DataFrame({\"score\": [10, 20, 30]})\n",
    "    \n",
    "    # Test normalization\n",
    "    df_norm = normalize_column(df, \"score\")\n",
    "    \n",
    "    assert df_norm[\"score\"].min() == 0.0\n",
    "    assert df_norm[\"score\"].max() == 1.0\n",
    "    assert len(df_norm) == 3\n",
    "\n",
    "def test_invalid_column():\n",
    "    df = pd.DataFrame({\"score\": [10, 20, 30]})\n",
    "    with pytest.raises(KeyError):\n",
    "        normalize_column(df, \"invalid_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c7a4a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /Users/macbookair/Documents/Data Science 5th Year/Advanced Programing for DS\n",
      "plugins: anyio-4.9.0, langsmith-0.4.8\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_normalization.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.66s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_normalization.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.66s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This is expected to fail or error because the module/function doesn't exist yet\n",
    "!pytest test_normalization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c1106",
   "metadata": {},
   "source": [
    "## Exercise 3: Testing Model Evaluation Function\n",
    "**Objective**: Test ML evaluation logic using pytest.\n",
    "\n",
    "**Task**: Write tests for `evaluate_model(y_true, y_pred)` that returns a dictionary with accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce75183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluation.py\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"f1_score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26cce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_evaluation.py\n",
    "import pytest\n",
    "from evaluation import evaluate_model\n",
    "\n",
    "def test_evaluate_model_perfect():\n",
    "    y_true = [1, 0, 1, 1]\n",
    "    y_pred = [1, 0, 1, 1]\n",
    "    metrics = evaluate_model(y_true, y_pred)\n",
    "    \n",
    "    assert metrics[\"accuracy\"] == 1.0\n",
    "    assert metrics[\"f1_score\"] == 1.0\n",
    "    assert \"accuracy\" in metrics\n",
    "    assert \"f1_score\" in metrics\n",
    "\n",
    "def test_evaluate_model_wrong():\n",
    "    y_true = [1, 0, 1]\n",
    "    y_pred = [0, 1, 0]\n",
    "    metrics = evaluate_model(y_true, y_pred)\n",
    "    \n",
    "    assert metrics[\"accuracy\"] == 0.0\n",
    "    assert metrics[\"f1_score\"] == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b374c780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /Users/macbookair/Documents/Data Science 5th Year/Advanced Programing for DS\n",
      "plugins: anyio-4.9.0, langsmith-0.4.8\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_evaluation.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 3.77s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f36c48",
   "metadata": {},
   "source": [
    "## Exercise 4: Continuous Integration with GitHub Actions\n",
    "**Objective**: Automate testing using GitHub workflows.\n",
    "\n",
    "**Task**: Create a `.github/workflows/run-tests.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ea9b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow file created at .github/workflows/run-tests.yml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\".github/workflows\", exist_ok=True)\n",
    "\n",
    "workflow_content = \"\"\"name: Run Tests\n",
    "\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v3\n",
    "\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup_python@v4\n",
    "      with:\n",
    "        python-version: '3.10'\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install pytest pandas scikit-learn\n",
    "\n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        pytest -v\n",
    "\"\"\"\n",
    "\n",
    "with open(\".github/workflows/run-tests.yml\", \"w\") as f:\n",
    "    f.write(workflow_content)\n",
    "\n",
    "print(\"Workflow file created at .github/workflows/run-tests.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18057f78",
   "metadata": {},
   "source": [
    "## Exercise 5: End-to-End Testing (Integration Test)\n",
    "**Objective**: Combine testing of multiple components.\n",
    "\n",
    "**Task**: Create and test a mini ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9d1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_integration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_integration.py\n",
    "import pandas as pd\n",
    "import pytest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evaluation import evaluate_model\n",
    "\n",
    "# Mock functions for the pipeline\n",
    "def load_data():\n",
    "    # Returns a simple dataframe\n",
    "    data = {\n",
    "        \"feature1\": [1, 2, 3, 4, 5, 6],\n",
    "        \"feature2\": [10, 20, 30, 40, 50, 60],\n",
    "        \"target\": [0, 0, 0, 1, 1, 1]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def train_model(X, y):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def test_ml_pipeline():\n",
    "    # 1. Load Data\n",
    "    df = load_data()\n",
    "    assert not df.empty\n",
    "    assert \"target\" in df.columns\n",
    "    \n",
    "    # 2. Train Model\n",
    "    X = df[[\"feature1\", \"feature2\"]]\n",
    "    y = df[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    model = train_model(X_train, y_train)\n",
    "    assert model is not None\n",
    "    \n",
    "    # 3. Evaluate Model\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = evaluate_model(y_test, y_pred)\n",
    "    \n",
    "    assert \"accuracy\" in metrics\n",
    "    assert 0 <= metrics[\"accuracy\"] <= 1.0\n",
    "    print(f\"Integration Test Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ddba9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /Users/macbookair/Documents/Data Science 5th Year/Advanced Programing for DS\n",
      "plugins: anyio-4.9.0, langsmith-0.4.8\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_integration.py \u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "test_integration.py::test_ml_pipeline\n",
      "  /Users/macbookair/.pyenv/versions/3.11.12/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:455: DeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n",
      "    opt_res = optimize.minimize(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m========================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 1.81s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_integration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d032e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
